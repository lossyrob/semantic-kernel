{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Postgres as memory\n",
    "\n",
    "This notebook shows how to use Postgres as a memory store in Semantic Kernel.\n",
    "\n",
    "The code below pulls the most recent papers from [ArviX](https://arxiv.org/), creates embeddings from the paper abstracts, and stores them in a Postgres database.\n",
    "\n",
    "In the future, we can use the Postgres vector store to search the database for similar papers based on the embeddings - stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Annotated, Any\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (\n",
    "    OpenAIEmbeddingPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding import AzureTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding import OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_db_for_postgres.azure_db_for_postgres_collection import (\n",
    "    AzureDBForPostgresCollection,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.postgres.postgres_collection import PostgresCollection\n",
    "from semantic_kernel.data.const import DistanceFunction, IndexKind\n",
    "from semantic_kernel.data.vector_store_model_decorator import vectorstoremodel\n",
    "from semantic_kernel.data.vector_store_record_fields import (\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    ")\n",
    "from semantic_kernel.data.vector_store_record_utils import VectorStoreRecordUtils\n",
    "from semantic_kernel.kernel import Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your environment\n",
    "\n",
    "You'll need to set up your environment to provide connection information to Postgres, as well as OpenAI or Azure OpenAI.\n",
    "\n",
    "To do this, copy the `.env.example` file to `.env` and fill in the necessary information.\n",
    "\n",
    "Note that if you are using VS Code to execute this notebook, ensure you don't have alternate values in the .env file at the root of the workspace, as that will take precedence over the .env file in the notebook.\n",
    "\n",
    "### Postgres configuration\n",
    "\n",
    "You'll need to provide a connection string to a Postgres database. You can use a local Postgres instance, or a cloud-hosted one.\n",
    "You can provide a connection string, or provide environment variables with the connection information. See the .env.example file for `POSTGRES_CONNECTION_STRING` and `PG*` settings.\n",
    "\n",
    "#### Using Azure DB for Postgres\n",
    "\n",
    "You can use Azure DB for Postgres by following the steps below:\n",
    "\n",
    "1. Create an Azure DB for Postgres instance. You can set the database to only allow Entra authentication to avoid\n",
    "    storing the password in the `.env` file.\n",
    "2. Set the `PG*` settings, except for the password if using Entra authentication. If using entra, ensure you\n",
    "    are logged in via the Azure CLI. You can get the configuration values from the Azure portal Settings -> Connect\n",
    "    page.\n",
    "3. Set \"USE_AZURE_DB_FOR_POSTGRES\" to True in the cell below.\n",
    "\n",
    "#### Using Docker\n",
    "\n",
    "You can also use docker to bring up a Postgres instance by following the steps below:\n",
    "\n",
    "Create an `init.sql` that has the following:\n",
    "\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "```\n",
    "\n",
    "Now you can start a postgres instance with the following:\n",
    "\n",
    "```\n",
    "docker pull pgvector/pgvector:pg16\n",
    "docker run --rm -it --name pgvector -p 5432:5432 -v ./init.sql:/docker-entrypoint-initdb.d/init.sql -e POSTGRES_PASSWORD=example pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "_Note_: Use `.\\init.sql` on Windows and `./init.sql` on WSL or Linux/Mac.\n",
    "\n",
    "Then you could use the connection string:\n",
    "\n",
    "```\n",
    "POSTGRES_CONNECTION_STRING=\"host=localhost port=5432 dbname=postgres user=postgres password=example\"\n",
    "```\n",
    "\n",
    "### OpenAI configuration\n",
    "\n",
    "You can either use OpenAI or Azure OpenAI APIs. You provide the API key and other configuration in the `.env` file. Set either the `OPENAI_` or `AZURE_OPENAI_` settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the environment file\n",
    "env_file_path = \".env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set some additional configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ArXiv settings --\n",
    "\n",
    "# The search term to use when searching for papers on arXiv. All metadata fields for the papers are searched.\n",
    "SEARCH_TERM = \"generative ai\"\n",
    "\n",
    "# The category of papers to search for on arXiv. See https://arxiv.org/category_taxonomy for a list of categories.\n",
    "ARVIX_CATEGORY = \"cs.AI\"\n",
    "\n",
    "# The maximum number of papers to search for on arXiv.\n",
    "MAX_RESULTS = 10\n",
    "\n",
    "# -- OpenAI settings --\n",
    "\n",
    "# Set this flag to False to use the OpenAI API instead of Azure OpenAI\n",
    "USE_AZURE_OPENAI = True\n",
    "\n",
    "# The name of the OpenAI model or Azure OpenAI deployment to use\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# -- Postgres settings --\n",
    "\n",
    "# Use Azure DB For Postgres. This enables Entra authentication against the database instead of\n",
    "# setting a password in the environment.\n",
    "USE_AZURE_DB_FOR_POSTGRES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a vector store model. This model defines the table and column names for storing the embeddings. We use the `@vectorstoremodel` decorator to tell Semantic Kernel to create a vector store definition from the model. The VectorStoreRecordField annotations define the fields that will be stored in the database, including key and vector fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class ArxivPaper:\n",
    "    id: Annotated[str, VectorStoreRecordKeyField()]\n",
    "    title: Annotated[str, VectorStoreRecordDataField()]\n",
    "    abstract: Annotated[str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"abstract_vector\")]\n",
    "    published: Annotated[datetime, VectorStoreRecordDataField()]\n",
    "    authors: Annotated[list[str], VectorStoreRecordDataField()]\n",
    "    link: Annotated[str | None, VectorStoreRecordDataField()]\n",
    "\n",
    "    abstract_vector: Annotated[\n",
    "        np.ndarray | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},\n",
    "            index_kind=IndexKind.HNSW,\n",
    "            dimensions=1536,\n",
    "            distance_function=DistanceFunction.COSINE,\n",
    "            property_type=\"float\",\n",
    "            serialize_function=np.ndarray.tolist,\n",
    "            deserialize_function=np.array,\n",
    "        ),\n",
    "    ] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_arxiv_info(cls, arxiv_info: dict[str, Any]) -> \"ArxivPaper\":\n",
    "        return cls(\n",
    "            id=arxiv_info[\"id\"],\n",
    "            title=arxiv_info[\"title\"].replace(\"\\n  \", \" \"),\n",
    "            abstract=arxiv_info[\"abstract\"].replace(\"\\n  \", \" \"),\n",
    "            published=arxiv_info[\"published\"],\n",
    "            authors=arxiv_info[\"authors\"],\n",
    "            link=arxiv_info[\"link\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that queries the ArviX API for the most recent papers based on our search query and category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_arxiv(search_query: str, category: str = \"cs.AI\", max_results: int = 10) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Query the ArXiv API and return a list of dictionaries with relevant metadata for each paper.\n",
    "\n",
    "    Args:\n",
    "        search_query: The search term or topic to query for.\n",
    "        category: The category to restrict the search to (default is \"cs.AI\").\n",
    "        See https://arxiv.org/category_taxonomy for a list of categories.\n",
    "        max_results: Maximum number of results to retrieve (default is 10).\n",
    "    \"\"\"\n",
    "    response = requests.get(\n",
    "        \"http://export.arxiv.org/api/query?\"\n",
    "        f\"search_query=all:%22{search_query.replace(' ', '+')}%22\"\n",
    "        f\"+AND+cat:{category}&start=0&max_results={max_results}&sortBy=lastUpdatedDate&sortOrder=descending\"\n",
    "    )\n",
    "\n",
    "    root = ET.fromstring(response.content)\n",
    "    ns = {\"atom\": \"http://www.w3.org/2005/Atom\"}\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"id\": entry.find(\"atom:id\", ns).text.split(\"/\")[-1],\n",
    "            \"title\": entry.find(\"atom:title\", ns).text,\n",
    "            \"abstract\": entry.find(\"atom:summary\", ns).text,\n",
    "            \"published\": entry.find(\"atom:published\", ns).text,\n",
    "            \"link\": entry.find(\"atom:id\", ns).text,\n",
    "            \"authors\": [author.find(\"atom:name\", ns).text for author in entry.findall(\"atom:author\", ns)],\n",
    "            \"categories\": [category.get(\"term\") for category in entry.findall(\"atom:category\", ns)],\n",
    "            \"pdf_link\": next(\n",
    "                (link_tag.get(\"href\") for link_tag in entry.findall(\"atom:link\", ns) if link_tag.get(\"title\") == \"pdf\"),\n",
    "                None,\n",
    "            ),\n",
    "        }\n",
    "        for entry in root.findall(\"atom:entry\", ns)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to query papers and store them in memory as our model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 papers on 'generative ai'\n"
     ]
    }
   ],
   "source": [
    "arxiv_papers: list[ArxivPaper] = [\n",
    "    ArxivPaper.from_arxiv_info(paper)\n",
    "    for paper in query_arxiv(SEARCH_TERM, category=ARVIX_CATEGORY, max_results=MAX_RESULTS)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(arxiv_papers)} papers on '{SEARCH_TERM}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `PostgresCollection`, which represents the table in Postgres where we will store the paper information and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_AZURE_DB_FOR_POSTGRES:\n\u001b[0;32m----> 2\u001b[0m     collection \u001b[38;5;241m=\u001b[39m \u001b[43mAzureDBForPostgresCollection\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mArxivPaper\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv_papers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_model_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mArxivPaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_file_path\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     collection \u001b[38;5;241m=\u001b[39m PostgresCollection[\u001b[38;5;28mstr\u001b[39m, ArxivPaper](\n\u001b[1;32m      7\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv_papers\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_model_type\u001b[38;5;241m=\u001b[39mArxivPaper, env_file_path\u001b[38;5;241m=\u001b[39menv_file_path\n\u001b[1;32m      8\u001b[0m     )\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/semantic_kernel/connectors/memory/azure_db_for_postgres/azure_db_for_postgres_collection.py:47\u001b[0m, in \u001b[0;36mAzureDBForPostgresCollection.__init__\u001b[0;34m(self, collection_name, data_model_type, data_model_definition, connection_pool, db_schema, env_file_path, env_file_encoding, settings)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# If the connection pool or settings were not provided, create the settings from the environment.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Passing this to the super class will enforce using Azure DB settings.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connection_pool \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings:\n\u001b[0;32m---> 47\u001b[0m     settings \u001b[38;5;241m=\u001b[39m \u001b[43mAzureDBForPostgresSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_file_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_file_encoding\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     51\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m     52\u001b[0m     data_model_type\u001b[38;5;241m=\u001b[39mdata_model_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m     57\u001b[0m )\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/semantic_kernel/kernel_pydantic.py:56\u001b[0m, in \u001b[0;36mKernelBaseSettings.create\u001b[0;34m(cls, **data)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_file_encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_file_encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m data \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/.venv/lib/python3.10/site-packages/pydantic_settings/main.py:144\u001b[0m, in \u001b[0;36mBaseSettings.__init__\u001b[0;34m(__pydantic_self__, _case_sensitive, _env_prefix, _env_file, _env_file_encoding, _env_ignore_empty, _env_nested_delimiter, _env_parse_none_str, _env_parse_enums, _cli_prog_name, _cli_parse_args, _cli_settings_source, _cli_parse_none_str, _cli_hide_none_type, _cli_avoid_json, _cli_enforce_required, _cli_use_class_docs_for_groups, _cli_exit_on_error, _cli_prefix, _secrets_dir, **values)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    121\u001b[0m     __pydantic_self__,\n\u001b[1;32m    122\u001b[0m     _case_sensitive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings_build_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_case_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_case_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_file_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_file_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_ignore_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_ignore_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_nested_delimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_nested_delimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_parse_none_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_parse_none_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_env_parse_enums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_env_parse_enums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_prog_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_prog_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_parse_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_parse_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_settings_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_settings_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_parse_none_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_parse_none_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_hide_none_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_hide_none_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_avoid_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_avoid_json\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_enforce_required\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_enforce_required\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_use_class_docs_for_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_use_class_docs_for_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_exit_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_exit_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_cli_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cli_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_secrets_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_secrets_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/.venv/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:292\u001b[0m, in \u001b[0;36minit_private_attributes\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    290\u001b[0m pydantic_private \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, private_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__private_attributes__\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 292\u001b[0m     default \u001b[38;5;241m=\u001b[39m \u001b[43mprivate_attr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PydanticUndefined:\n\u001b[1;32m    294\u001b[0m         pydantic_private[name] \u001b[38;5;241m=\u001b[39m default\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/.venv/lib/python3.10/site-packages/pydantic/fields.py:936\u001b[0m, in \u001b[0;36mModelPrivateAttr.get_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    927\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the default value of the object.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m        The default value of the object.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory()\n",
      "File \u001b[0;32m~/proj/sk/semantic-kernel/python/.venv/lib/python3.10/site-packages/pydantic/_internal/_utils.py:318\u001b[0m, in \u001b[0;36msmart_deepcopy\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# do we really dare to catch ALL errors? Seems a bit risky\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.15-linux-x86_64-gnu/lib/python3.10/copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "if USE_AZURE_DB_FOR_POSTGRES:\n",
    "    collection = AzureDBForPostgresCollection[str, ArxivPaper](\n",
    "        collection_name=\"arxiv_papers\", data_model_type=ArxivPaper, env_file_path=env_file_path\n",
    "    )\n",
    "else:\n",
    "    collection = PostgresCollection[str, ArxivPaper](\n",
    "        collection_name=\"arxiv_papers\", data_model_type=ArxivPaper, env_file_path=env_file_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Kernel and add the TextEmbedding service, which will be used to generate embeddings of the abstract for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "if USE_AZURE_OPENAI:\n",
    "    text_embedding = AzureTextEmbedding(\n",
    "        service_id=\"embedding\", deployment_name=EMBEDDING_MODEL, env_file_path=env_file_path\n",
    "    )\n",
    "else:\n",
    "    text_embedding = OpenAITextEmbedding(\n",
    "        service_id=\"embedding\", ai_model_id=EMBEDDING_MODEL, env_file_path=env_file_path\n",
    "    )\n",
    "\n",
    "kernel.add_service(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use VectorStoreRecordUtils to add embeddings to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = await VectorStoreRecordUtils(kernel).add_vector_to_records(arxiv_papers, data_model_type=ArxivPaper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the models have embeddings, we can write them into the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC TOKEN:  eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ik1jN2wzSXo5M2c3dXdnTmVFbW13X1dZR1BrbyIsImtpZCI6Ik1jN2wzSXo5M2c3dXdnTmVFbW13X1dZR1BrbyJ9.eyJhdWQiOiJodHRwczovL29zc3JkYm1zLWFhZC5kYXRhYmFzZS53aW5kb3dzLm5ldCIsImlzcyI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzcyZjk4OGJmLTg2ZjEtNDFhZi05MWFiLTJkN2NkMDExZGI0Ny8iLCJpYXQiOjE3Mjc3MTE3OTQsIm5iZiI6MTcyNzcxMTc5NCwiZXhwIjoxNzI3NzE2NzU1LCJfY2xhaW1fbmFtZXMiOnsiZ3JvdXBzIjoic3JjMSJ9LCJfY2xhaW1fc291cmNlcyI6eyJzcmMxIjp7ImVuZHBvaW50IjoiaHR0cHM6Ly9ncmFwaC53aW5kb3dzLm5ldC83MmY5ODhiZi04NmYxLTQxYWYtOTFhYi0yZDdjZDAxMWRiNDcvdXNlcnMvNGEyMzRmMDUtYjkzMC00NDBlLTkyNjMtZjVkYjFlZDhmNDRhL2dldE1lbWJlck9iamVjdHMifX0sImFjciI6IjEiLCJhaW8iOiJBVlFBcS84WUFBQUE2aWlkY3lVT1lGS0gycktQRHRTU3Qzc1l1dzY2cXBUWHRsWFJuVUpFZVNVblNHRm4yQ2R1MGFKQTVjTUFEMUVNR2hyNzd5dzZCd2o3WUthVndYajRwTFNkdDJydW1TSHhGQWc2L3NmQnVhMD0iLCJhbXIiOlsicHdkIiwicnNhIiwibWZhIl0sImFwcGlkIjoiMDRiMDc3OTUtOGRkYi00NjFhLWJiZWUtMDJmOWUxYmY3YjQ2IiwiYXBwaWRhY3IiOiIwIiwiZGV2aWNlaWQiOiJhNDdhZGE2Yi0zOGRhLTQ3MDItOWZhNy0wMWI5ZWI2OWEwMWUiLCJmYW1pbHlfbmFtZSI6IkVtYW51ZWxlIiwiZ2l2ZW5fbmFtZSI6IlJvYiIsImlkdHlwIjoidXNlciIsImlwYWRkciI6IjcxLjE3NS4xMzcuMTI1IiwibmFtZSI6IlJvYiBFbWFudWVsZSIsIm9pZCI6IjRhMjM0ZjA1LWI5MzAtNDQwZS05MjYzLWY1ZGIxZWQ4ZjQ0YSIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0xMjQ1MjUwOTUtNzA4MjU5NjM3LTE1NDMxMTkwMjEtMjAxMDY2NiIsInB1aWQiOiIxMDAzMjAwMEYxRDk4NUZCIiwicmgiOiIxLkFSb0F2NGo1Y3ZHR3IwR1JxeTE4MEJIYlIxRFlQQkxmMmIxQWxOWEo4SHRfb2dNYUFHSWFBQS4iLCJzY3AiOiJ1c2VyX2ltcGVyc29uYXRpb24iLCJzdWIiOiJoS0lKSkRHeGtldXh4R1hnYjlMWlJ3RlZ6cDAyek5taTQwb20wWWJ3YWEwIiwidGlkIjoiNzJmOTg4YmYtODZmMS00MWFmLTkxYWItMmQ3Y2QwMTFkYjQ3IiwidW5pcXVlX25hbWUiOiJyb2JlbWFudWVsZUBtaWNyb3NvZnQuY29tIiwidXBuIjoicm9iZW1hbnVlbGVAbWljcm9zb2Z0LmNvbSIsInV0aSI6Ik5CblFnNy1YZVVpNTI5QmxDMFlEQUEiLCJ2ZXIiOiIxLjAiLCJ4bXNfaWRyZWwiOiIyIDEifQ.vEevMMADNaEW0J2yDTWpmHzZx2oupvHOJ5sDXSBCaK8qUdKEevyzzKFL9qeV0D5hYPwxR4EE37oJJImliTPoiCstHUApgu_jfkQzXdqWRNxLquHiAreFzcOCWXWlJLA9Vlv5V9PbnIWScoa8Alv3OBH4uPDTyIGDuFZ7DYfjiYNWydJL-P2zwWVh3Ks6ODfVLzCq64dDx6gsIZW684Ou_aRwKwEb86cB6zcR00peQ4uzyULMjCJm7IbyJTPUI9ijkU6ezNgbUQJUqSkJgcmOilZsp_p3OMwHXdXoCCYaOiI3EVh1JPkSo9VP3015W5xW_NvSujPeteqlFkxgiVDr5Q\n"
     ]
    }
   ],
   "source": [
    "async with collection:\n",
    "    await collection.create_collection_if_not_exists()\n",
    "    keys = await collection.upsert_batch(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we retrieve the first few models from the database and print out their information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC TOKEN:  eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ik1jN2wzSXo5M2c3dXdnTmVFbW13X1dZR1BrbyIsImtpZCI6Ik1jN2wzSXo5M2c3dXdnTmVFbW13X1dZR1BrbyJ9.eyJhdWQiOiJodHRwczovL29zc3JkYm1zLWFhZC5kYXRhYmFzZS53aW5kb3dzLm5ldCIsImlzcyI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzcyZjk4OGJmLTg2ZjEtNDFhZi05MWFiLTJkN2NkMDExZGI0Ny8iLCJpYXQiOjE3Mjc3MTE3OTQsIm5iZiI6MTcyNzcxMTc5NCwiZXhwIjoxNzI3NzE2NzU1LCJfY2xhaW1fbmFtZXMiOnsiZ3JvdXBzIjoic3JjMSJ9LCJfY2xhaW1fc291cmNlcyI6eyJzcmMxIjp7ImVuZHBvaW50IjoiaHR0cHM6Ly9ncmFwaC53aW5kb3dzLm5ldC83MmY5ODhiZi04NmYxLTQxYWYtOTFhYi0yZDdjZDAxMWRiNDcvdXNlcnMvNGEyMzRmMDUtYjkzMC00NDBlLTkyNjMtZjVkYjFlZDhmNDRhL2dldE1lbWJlck9iamVjdHMifX0sImFjciI6IjEiLCJhaW8iOiJBVlFBcS84WUFBQUE2aWlkY3lVT1lGS0gycktQRHRTU3Qzc1l1dzY2cXBUWHRsWFJuVUpFZVNVblNHRm4yQ2R1MGFKQTVjTUFEMUVNR2hyNzd5dzZCd2o3WUthVndYajRwTFNkdDJydW1TSHhGQWc2L3NmQnVhMD0iLCJhbXIiOlsicHdkIiwicnNhIiwibWZhIl0sImFwcGlkIjoiMDRiMDc3OTUtOGRkYi00NjFhLWJiZWUtMDJmOWUxYmY3YjQ2IiwiYXBwaWRhY3IiOiIwIiwiZGV2aWNlaWQiOiJhNDdhZGE2Yi0zOGRhLTQ3MDItOWZhNy0wMWI5ZWI2OWEwMWUiLCJmYW1pbHlfbmFtZSI6IkVtYW51ZWxlIiwiZ2l2ZW5fbmFtZSI6IlJvYiIsImlkdHlwIjoidXNlciIsImlwYWRkciI6IjcxLjE3NS4xMzcuMTI1IiwibmFtZSI6IlJvYiBFbWFudWVsZSIsIm9pZCI6IjRhMjM0ZjA1LWI5MzAtNDQwZS05MjYzLWY1ZGIxZWQ4ZjQ0YSIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0xMjQ1MjUwOTUtNzA4MjU5NjM3LTE1NDMxMTkwMjEtMjAxMDY2NiIsInB1aWQiOiIxMDAzMjAwMEYxRDk4NUZCIiwicmgiOiIxLkFSb0F2NGo1Y3ZHR3IwR1JxeTE4MEJIYlIxRFlQQkxmMmIxQWxOWEo4SHRfb2dNYUFHSWFBQS4iLCJzY3AiOiJ1c2VyX2ltcGVyc29uYXRpb24iLCJzdWIiOiJoS0lKSkRHeGtldXh4R1hnYjlMWlJ3RlZ6cDAyek5taTQwb20wWWJ3YWEwIiwidGlkIjoiNzJmOTg4YmYtODZmMS00MWFmLTkxYWItMmQ3Y2QwMTFkYjQ3IiwidW5pcXVlX25hbWUiOiJyb2JlbWFudWVsZUBtaWNyb3NvZnQuY29tIiwidXBuIjoicm9iZW1hbnVlbGVAbWljcm9zb2Z0LmNvbSIsInV0aSI6Ik5CblFnNy1YZVVpNTI5QmxDMFlEQUEiLCJ2ZXIiOiIxLjAiLCJ4bXNfaWRyZWwiOiIyIDEifQ.vEevMMADNaEW0J2yDTWpmHzZx2oupvHOJ5sDXSBCaK8qUdKEevyzzKFL9qeV0D5hYPwxR4EE37oJJImliTPoiCstHUApgu_jfkQzXdqWRNxLquHiAreFzcOCWXWlJLA9Vlv5V9PbnIWScoa8Alv3OBH4uPDTyIGDuFZ7DYfjiYNWydJL-P2zwWVh3Ks6ODfVLzCq64dDx6gsIZW684Ou_aRwKwEb86cB6zcR00peQ4uzyULMjCJm7IbyJTPUI9ijkU6ezNgbUQJUqSkJgcmOilZsp_p3OMwHXdXoCCYaOiI3EVh1JPkSo9VP3015W5xW_NvSujPeteqlFkxgiVDr5Q\n",
      "# Data Analysis in the Era of Generative AI\n",
      "\n",
      "Abstract:   This paper explores the potential of AI-powered tools to reshape data\n",
      "analysis, focusing on design considerations and challenges. We explore how the\n",
      "emergence of large language and multimodal models offers new opportunities to\n",
      "enhance various stages of data analysis workflow by translating high-level user\n",
      "intentions into executable code, charts, and insights. We then examine human-\n",
      "centered design principles that facilitate intuitive interactions, build user\n",
      "trust, and streamline the AI-assisted analysis workflow across multiple apps.\n",
      "Finally, we discuss the research challenges that impede the development of these\n",
      "AI-based systems such as enhancing model capabilities, evaluating and\n",
      "benchmarking, and understanding end-user needs.\n",
      "Published: 2024-09-27 06:31:03\n",
      "Link: http://arxiv.org/abs/2409.18475v1\n",
      "PDF Link: http://arxiv.org/abs/2409.18475v1\n",
      "Authors: Jeevana Priya Inala, Chenglong Wang, Steven Drucker, Gonzalo Ramos, Victor Dibia, Nathalie Riche, Dave Brown, Dan Marshall, Jianfeng Gao\n",
      "Embedding: [ 0.0324665   0.03064382  0.04381268 ... -0.00220003 -0.01004753\n",
      "  0.0266567 ]\n",
      "\n",
      "\n",
      "# Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly\n",
      "\n",
      "Abstract:   We present a system that transforms speech into physical objects by combining\n",
      "3D generative Artificial Intelligence with robotic assembly. The system\n",
      "leverages natural language input to make design and manufacturing more\n",
      "accessible, enabling individuals without expertise in 3D modeling or robotic\n",
      "programming to create physical objects. We propose utilizing discrete robotic\n",
      "assembly of lattice-based voxel components to address the challenges of using\n",
      "generative AI outputs in physical production, such as design variability,\n",
      "fabrication speed, structural integrity, and material waste. The system\n",
      "interprets speech to generate 3D objects, discretizes them into voxel\n",
      "components, computes an optimized assembly sequence, and generates a robotic\n",
      "toolpath. The results are demonstrated through the assembly of various objects,\n",
      "ranging from chairs to shelves, which are prompted via speech and realized\n",
      "within 5 minutes using a 6-axis robotic arm.\n",
      "Published: 2024-09-27 02:12:56\n",
      "Link: http://arxiv.org/abs/2409.18390v1\n",
      "PDF Link: http://arxiv.org/abs/2409.18390v1\n",
      "Authors: Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld\n",
      "Embedding: [ 0.00662689  0.03477224  0.01948195 ...  0.02736436 -0.00416199\n",
      "  0.01341196]\n",
      "\n",
      "\n",
      "# Deep Generative Model for Mechanical System Configuration Design\n",
      "\n",
      "Abstract:   Generative AI has made remarkable progress in addressing various design\n",
      "challenges. One prominent area where generative AI could bring significant value\n",
      "is in engineering design. In particular, selecting an optimal set of components\n",
      "and their interfaces to create a mechanical system that meets design\n",
      "requirements is one of the most challenging and time-consuming tasks for\n",
      "engineers. This configuration design task is inherently challenging due to its\n",
      "categorical nature, multiple design requirements a solution must satisfy, and\n",
      "the reliance on physics simulations for evaluating potential solutions. These\n",
      "characteristics entail solving a combinatorial optimization problem with\n",
      "multiple constraints involving black-box functions. To address this challenge,\n",
      "we propose a deep generative model to predict the optimal combination of\n",
      "components and interfaces for a given design problem. To demonstrate our\n",
      "approach, we solve a gear train synthesis problem by first creating a synthetic\n",
      "dataset using a grammar, a parts catalogue, and a physics simulator. We then\n",
      "train a Transformer using this dataset, named GearFormer, which can not only\n",
      "generate quality solutions on its own, but also augment search methods such as\n",
      "an evolutionary algorithm and Monte Carlo tree search. We show that GearFormer\n",
      "outperforms such search methods on their own in terms of satisfying the\n",
      "specified design requirements with orders of magnitude faster generation time.\n",
      "Additionally, we showcase the benefit of hybrid methods that leverage both\n",
      "GearFormer and search methods, which further improve the quality of the\n",
      "solutions.\n",
      "Published: 2024-09-09 19:15:45\n",
      "Link: http://arxiv.org/abs/2409.06016v2\n",
      "PDF Link: http://arxiv.org/abs/2409.06016v2\n",
      "Authors: Yasaman Etesam, Hyunmin Cheong, Mohammadmehdi Ataei, Pradeep Kumar Jayaraman\n",
      "Embedding: [0.02961224 0.01000344 0.03739412 ... 0.00147997 0.02606801 0.044123  ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async with collection:\n",
    "    results = await collection.get_batch(keys[:3])\n",
    "    if results:\n",
    "        for result in results:\n",
    "            print(f\"# {result.title}\")\n",
    "            print()\n",
    "            wrapped_abstract = textwrap.fill(result.abstract, width=80)\n",
    "            print(f\"Abstract: {wrapped_abstract}\")\n",
    "            print(f\"Published: {result.published}\")\n",
    "            print(f\"Link: {result.link}\")\n",
    "            print(f\"PDF Link: {result.link}\")\n",
    "            print(f\"Authors: {', '.join(result.authors)}\")\n",
    "            print(f\"Embedding: {result.abstract_vector}\")\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...searching Postgres memory coming soon, to be continued!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
